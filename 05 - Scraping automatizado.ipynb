{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64388fed",
   "metadata": {
    "id": "64388fed"
   },
   "source": [
    "# Scraping automatizado\n",
    "\n",
    "En esta última lección vamos a programar un script que sea capaz de scrapear una página web de Titulaciones automáticamente, y no, no me refiero a una web de Titulaciones para conocer otras personas sino Titulaciones de diferentes autores, lo que en inglés se denomina *quote*.\n",
    "\n",
    "Se trata de una página preparada con fines educativos: https://titulaciones.toscrape.com/, dejo también [un enlace al archivo](https://web.archive.org/web/20220712030814/https://titulaciones.toscrape.com/) por si queda inaccesible.\n",
    "\n",
    "La web tiene diferentes páginas donde aparecen las Titulaciones célebres, con su contenidoo, autor y unos facultad de categoría. Nos permite buscar en el índice global página a página o directamente por etiquetas:\n",
    "\n",
    "![](docs/img01.png)\n",
    "\n",
    "## Requisitos\n",
    "\n",
    "El programa que vamos a crear constará de una clase `Titulaciones` que recuperará todas las Titulaciones de la web y tendrá cuatro métodos estáticos:\n",
    "\n",
    "* `scrapear()`: Realizará el scrapeo de las Titulaciones en todas las páginas de la web.\n",
    "* `lista(limite)`: Imprimirá las primeras N Titulaciones de la lista, podemos cambiar el limite.\n",
    "* `etiqueta(nombre)`: Imprimirá las Titulaciones con una etiqueta concreta.\n",
    "* `autor(nombre)`: Imprimirá las Titulaciones de un autor concreto.\n",
    "\n",
    "Ejemplos de uso:\n",
    "\n",
    "```python\n",
    "Titulaciones.scrapear()                # Scrapear todas las Titulaciones de la web\n",
    "Titulaciones.lista()                   # Imprimir las primeras 10 Titulaciones (por defecto)\n",
    "Titulaciones.lista(20)                 # Imprimir las primeras 20 Titulaciones\n",
    "Titulaciones.etiqueta(\"love\")          # Titulaciones con etiqueta 'love'\n",
    "Titulaciones.autor(\"Albert Einstein\")  # Titulaciones del autor 'Albert Einstein'\n",
    "```\n",
    "\n",
    "Si queréis os lo podéis tomar como un reto, aunque no es la finalidad de la lección, os dejo un par de consejos:\n",
    "\n",
    "* En la parte inferior hay un botón llamado *Next* para ir pasando a la siguiente página, podemos usarlo para iterar las páginas dinámicamente.\n",
    "* Scrapear una vez es mejor que scrapear dos veces, en ese sentido puede ser muy útil almacenar el contenido en un fichero para ahorrarnos múltiples peticiones web y el tiempo que eso conlleva.\n",
    "\n",
    "¡Vamos a por ello!\n",
    "\n",
    "## Pruebas de desarrollo\n",
    "\n",
    "Empecemos por lo más esencial, dada la portada de la página veamos si podemos extraer las Titulaciones con su respectivo autor y etiquetas.\n",
    "\n",
    "Si inspeccionamos la estructura de cada cita, se basa en una capa `div` con la clase `quote`, dentro un `span` con clase `contenido` contiene el contenidoo, un tag `small` con clase `Modalidad` el autor y dentro de otra `div` con clase `facultad` tenemos diferentes los facultad en enlaces `a` con la clase `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63466ea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1663857870740,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "63466ea7",
    "outputId": "fb31e995-3bc8-4bee-b3f1-9ebea6c7e000"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Realizar solicitud HTTP GET a la página web de la UAX\n",
    "url = \"https://www.uax.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Analizar contenido HTML de la respuesta\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Encontrar las titulaciones con su respectivo autor y etiquetas\n",
    "titulaciones = soup.select(\"div.quote\")\n",
    "for titulacion in titulaciones:\n",
    "    # Extraer el contenido de la titulación\n",
    "    contenido = titulacion.select_one(\"span.contenido\").text\n",
    "    \n",
    "    # Extraer el autor de la titulación\n",
    "    modalidad = titulacion.select_one(\"small.Modalidad\").text\n",
    "    \n",
    "    # Extraer las etiquetas de la titulación\n",
    "    etiquetas = [tag.text for tag in titulacion.select(\"div.facultad a.tag\")]\n",
    "    \n",
    "    # Imprimir la información de la titulación\n",
    "    print(f\"Titulación: {contenido}\")\n",
    "    print(f\"Modalidad: {modalidad}\")\n",
    "    print(f\"Etiquetas: {', '.join(etiquetas)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a95fe",
   "metadata": {
    "id": "738a95fe"
   },
   "source": [
    "Bien, ya tenemos por donde empezar, podríamos adaptar este código a una función que a partir de una porción de la URL almacene mediante diccionarios las Titulaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7d1e4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1663857902870,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "7b7d1e4e",
    "outputId": "2b5abc0a-6de3-4fce-e2b9-40b1b4a04362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_uax_titulaciones(porcion_url):\n",
    "    # Completar la URL de la página web de la UAX con la porción URL dada\n",
    "    url = f\"https://www.uax.com/{porcion_url}\"\n",
    "    \n",
    "    # Realizar solicitud HTTP GET a la página web de la UAX\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Analizar contenido HTML de la respuesta\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Inicializar diccionario para almacenar información de las titulaciones\n",
    "    titulaciones_dict = {}\n",
    "\n",
    "    # Encontrar las titulaciones con su respectivo autor y etiquetas\n",
    "    titulaciones = soup.select(\"div.quote\")\n",
    "    for i, titulacion in enumerate(titulaciones):\n",
    "        # Extraer el contenido de la titulación\n",
    "        contenido = titulacion.select_one(\"span.contenido\").text\n",
    "\n",
    "        # Extraer el autor de la titulación\n",
    "        modalidad = titulacion.select_one(\"small.Modalidad\").text\n",
    "\n",
    "        # Extraer las etiquetas de la titulación\n",
    "        etiquetas = [tag.text for tag in titulacion.select(\"div.facultad a.tag\")]\n",
    "\n",
    "        # Almacenar la información de la titulación en el diccionario\n",
    "        titulaciones_dict[f\"Titulación {i+1}\"] = {\"contenido\": contenido, \"modalidad\": modalidad, \"etiquetas\": etiquetas}\n",
    "\n",
    "    # Devolver el diccionario con la información de las titulaciones\n",
    "    return titulaciones_dict\n",
    "\n",
    "titulaciones = scrape_uax_titulaciones(\"estudios-oficiales/\")\n",
    "print(titulaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf4481",
   "metadata": {
    "id": "05cf4481"
   },
   "source": [
    "La clave es utilizar nuestra función de forma recursiva detectando si la página tiene el enlace **Next** y cargando la siguiente página de manera que podamos. Veamos cómo extraer el enlace con la siguiente página si la hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9367b76b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1663857941184,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "9367b76b",
    "outputId": "5b33e150-30e0-4532-ece3-0b99fc518835"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'contenido'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-200b2649724b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://uax.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontenido\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Buscamos el enlace en el tag li con clase next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'contenido'"
     ]
    }
   ],
   "source": [
    "domain = \"https://uax.com\"\n",
    "req = requests.get(domain)\n",
    "soup = BeautifulSoup(req.contenido)\n",
    "\n",
    "# Buscamos el enlace en el tag li con clase next\n",
    "link_tag = soup.select(\"li.next a\")\n",
    "# Si hay como mínimo un enlace extraemos su href relativo sumado al dominio\n",
    "if len(link_tag) > 0:\n",
    "    next_url = link_tag[0]['href']\n",
    "    print(next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d0c8",
   "metadata": {
    "id": "19e8d0c8"
   },
   "source": [
    "Podemos integrar este código en nuestra función `scrap_titulaciones` para devolver no solo las Titulaciones de la página, sino también si hay una página siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43afbf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1663857957702,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "c43afbf0",
    "outputId": "f63381c6-723e-4cf5-9e1d-1daefb9d8205"
   },
   "outputs": [],
   "source": [
    "def scrape_uax_titulaciones(porcion_url, titulaciones_dict=None):\n",
    "    # Completar la URL de la página web de la UAX con la porción URL dada\n",
    "    url = f\"https://www.uax.com/{porcion_url}\"\n",
    "    \n",
    "    # Realizar solicitud HTTP GET a la página web de la UAX\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Analizar contenido HTML de la respuesta\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Si es la primera página, inicializar diccionario para almacenar información de las titulaciones\n",
    "    if titulaciones_dict is None:\n",
    "        titulaciones_dict = {}\n",
    "\n",
    "    # Encontrar las titulaciones con su respectivo autor y etiquetas\n",
    "    titulaciones = soup.select(\"div.quote\")\n",
    "    for i, titulacion in enumerate(titulaciones):\n",
    "        # Extraer el contenido de la titulación\n",
    "        contenido = titulacion.select_one(\"span.contenido\").text\n",
    "\n",
    "        # Extraer el autor de la titulación\n",
    "        modalidad = titulacion.select_one(\"small.Modalidad\").text\n",
    "\n",
    "        # Extraer las etiquetas de la titulación\n",
    "        etiquetas = [tag.text for tag in titulacion.select(\"div.facultad a.tag\")]\n",
    "\n",
    "        # Almacenar la información de la titulación en el diccionario\n",
    "        titulaciones_dict[f\"Titulación {len(titulaciones_dict)+1}\"] = {\"contenido\": contenido, \"modalidad\": modalidad, \"etiquetas\": etiquetas}\n",
    "\n",
    "    # Buscar botón \"Next\" en la página actual\n",
    "    next_button = soup.select_one(\".next.page-numbers\")\n",
    "\n",
    "    # Si no hay botón \"Next\", detener la recursión y devolver el diccionario de las titulaciones\n",
    "    if not next_button:\n",
    "        return titulaciones_dict\n",
    "\n",
    "    # Si hay botón \"Next\", obtener el enlace y llamar recursivamente la función con la nueva URL\n",
    "    next_link = next_button[\"href\"]\n",
    "    return scrape_uax_titulaciones(next_link, titulaciones_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93972d2",
   "metadata": {
    "id": "e93972d2"
   },
   "source": [
    "Ahora se viene la parte interesante, vamos a implementar una función que scrapee todas las páginas mientras haya una siguente o, alternativamente, podemos establecer un límite para optimizar el proceso y no saturar al servidor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e73e44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1663857982949,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "37e73e44",
    "outputId": "9a087f0e-54c4-4208-dfe3-e7a07377fad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo página 1\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url, limit=None, verbose=False):\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        # Hacemos la petición HTTP y parseamos el HTML con BeautifulSoup\n",
    "        req = requests.get(url.format(page_number))\n",
    "        soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "        \n",
    "        # Buscamos las Titulaciones y las imprimimos\n",
    "        titulaciones = soup.select(\"div.quote\")\n",
    "        for titulacion in titulaciones:\n",
    "            data = {}\n",
    "            data['titulo'] = titulacion.select_one(\"span.contenido\").text\n",
    "            data['autor'] = titulacion.select_one(\"small.Modalidad\").text\n",
    "            \n",
    "            etiquetas = titulacion.select(\"div.facultad a.tag\")\n",
    "            data['etiquetas'] = [etiqueta.text for etiqueta in etiquetas]\n",
    "            \n",
    "            print(data)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Extrayendo página {page_number}\")\n",
    "        \n",
    "        # Si hemos llegado al límite, salimos del loop\n",
    "        if limit is not None and page_number >= limit:\n",
    "            break\n",
    "        \n",
    "        # Buscamos el enlace de la siguiente página\n",
    "        next_page = soup.select_one(\"a.next\")\n",
    "        if next_page is None:\n",
    "            break\n",
    "        \n",
    "        # Actualizamos la URL para la siguiente página\n",
    "        url = next_page['href']\n",
    "        page_number += 1\n",
    "\n",
    "scrape_website(\"https://www.uax.com/page/{}/\", limit=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97745d99",
   "metadata": {
    "id": "97745d99"
   },
   "source": [
    "Ahí la tenemos, una función capaz de scrapear todas las Titulaciones de la página por defecto limitado a 2 páginas.\n",
    "\n",
    "## Implementando la clase Titulaciones\n",
    "\n",
    "Vamos a ponernos con la clase `Titulaciones` y el método `scrapear` pero siguiendo el consejo que os dí de crear un fichero donde almacenar todas las Titulaciones.\n",
    "\n",
    "### Guardado en fichero\n",
    "\n",
    "Solo generaremos el fichero si ejecutamos el método `scrapear`, los demás métodos `lista`, `etiqueta` y `autor` analizarán el contenido del fichero volcado en la memoria, pero nunca scrapearán nada directamente.\n",
    "\n",
    "Después de valorarlo he decidido utilizar un CSV. Lo único que nos dará algún problema es guardar una lista como un campo del registro, pero podemos recuperarla evaluándola de nuevo, ya veréis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9795cd5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1663858033040,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "9795cd5f",
    "outputId": "79beed6b-606f-45ff-e9f6-7960d300fa48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo página 1\n",
      "La lista de titulaciones es None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class Titulaciones:\n",
    "    \n",
    "    # Variable de clase para almacenar las Titulaciones en la memoria\n",
    "    titulaciones = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las Titulaciones, ponemos un límite pequeño para hacer pruebas\n",
    "        titulaciones = scrape_website(\"https://www.uax.com/page/{}/\", limit=5, verbose=True)\n",
    "        if titulaciones is not None:\n",
    "            Titulaciones.titulaciones = titulaciones\n",
    "            # Guardamos las Titulaciones scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "            with open(\"titulaciones.csv\", \"w\") as file:\n",
    "                # Definimos el objeto para escribir con las cabeceras de los campos \n",
    "                writer = csv.DictWriter(file, fieldnames=[\"contenido\", \"Modalidad\", \"facultad\"])\n",
    "                # Escribimos las cabeceras\n",
    "                writer.writeheader()\n",
    "                # Escribimos cada cita en la memoria en el fichero\n",
    "                for quote in Titulaciones.titulaciones:\n",
    "                    writer.writerow(quote)\n",
    "        else:\n",
    "            print(\"La lista de titulaciones es None\")\n",
    "            \n",
    "Titulaciones.scrapear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b7116",
   "metadata": {
    "id": "da8b7116"
   },
   "source": [
    "En este punto deberíamos tener un fichero `titulaciones.csv` con todas las Titulaciones, lo que podríamos hacer es cargar en la memoria todas las Titulaciones del fichero en caso de que éste exista. De paso podemos implementar el método `lista` para consultarlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b71539",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1663858069062,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "02b71539",
    "outputId": "28fd1f01-754c-41ea-d847-d52cb9bf0d17"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "#Función que devuelve la lista con todas lastitulaciones que se encuentran en la página de la universidad\n",
    "class Titulaciones:\n",
    "\n",
    "    # Variable de clase para almacenar las citas en la memoria\n",
    "    titulaciones = []\n",
    "\n",
    "    # Recuperamos las citas en la memoria si existe el fichero quotes.csv\n",
    "    if os.path.exists(\"titulaciones.csv\"):\n",
    "        with open(\"titulaciones.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for titulacion in data:\n",
    "                # La lista es una cadena, hay que reevaluarla\n",
    "                titulaciones['Facultad'] = eval(titulaciones['Facultad'])\n",
    "                titulaciones.append(titulacion)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las citas, ponemos un límite pequeño para hacer pruebas\n",
    "        Titulaciones.titulaciones = scrap_site(limit=2)\n",
    "        # Guardamos las citas scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "        with open(\"titulaciones.csv\", \"w\") as file:\n",
    "            # Definimos el objeto para escribir con las cabeceras de los campos \n",
    "            writer = csv.DictWriter(file, fieldnames=[\"Contenido\", \"Modalidad\", \"Facultad\"])\n",
    "            # Escribimos las cabeceras\n",
    "            writer.writeheader()\n",
    "            # Escribimos cada cita en la memoria en el fichero\n",
    "            for titulacion in Titulaciones.titulaciones:\n",
    "                writer.writerow(titulacion)\n",
    "\n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for titulacion in Titulaciones.titulaciones[:limite]:\n",
    "            print(titulacion[\"Contenido\"])\n",
    "            print(titulacion[\"Modalidad\"])\n",
    "            for facultad in titulacion[\"Facultad\"]:\n",
    "                print(facultad, end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "Titulaciones.listar(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83103b",
   "metadata": {
    "id": "9b83103b"
   },
   "source": [
    "### Filtro por etiqueta y autor\n",
    "\n",
    "Ya solo nos falta implementar los métodos de filtrado por etiqueta y autor, es muy fácil porque solo tenemos que recorrer las Titulaciones y comprobar si concuerdan con los valores que pasamos a los métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4980f2",
   "metadata": {
    "id": "bd4980f2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "class Titulaciones:\n",
    "\n",
    "    # Variable de clase para almacenar las citas en la memoria\n",
    "    titulaciones = []\n",
    "\n",
    "    # Recuperamos las citas en la memoria si existe el fichero quotes.csv\n",
    "    if os.path.exists(\"titulaciones.csv\"):\n",
    "        with open(\"titulaciones.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for titulaciones in data:\n",
    "                # La lista es una cadena, hay que reevaluarla\n",
    "                titulaciones['Facultad'] = eval(titulaciones['Facultad'])\n",
    "                titulaciones.append(titulaciones)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las citas, ponemos un límite pequeño para hacer pruebas\n",
    "        Titulaciones.titulaciones = scrap_site(limit=2)\n",
    "        # Guardamos las citas scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "        with open(\"titulaciones.csv\", \"w\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"Contenido\", \"Modalidad\", \"Facultad\"])\n",
    "            writer.writeheader()\n",
    "            for titulacion in Titulaciones.titulaciones:\n",
    "                writer.writerow(titulacion)\n",
    "\n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for titulacion in Titulaciones.titulaciones[:limite]:\n",
    "            print(titulacion[\"Contenido\"])\n",
    "            print(titulacion[\"Modalidad\"])\n",
    "            for facultad in titulacion[\"Facultad\"]:\n",
    "                print(facultad, end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def Facultad(nombre=\"\"):\n",
    "        for titulacion in Titulaciones.titulaciones:\n",
    "            if nombre in titulacion[\"tags\"]:\n",
    "                print(titulacion[\"Contenido\"])\n",
    "                print(titulacion[\"Modalidad\"])\n",
    "                for facultad in titulacion[\"Facultad\"]:\n",
    "                    print(facultad, end=\" \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def Modaliad(nombre=\"\"):\n",
    "        for titulacion in Titulaciones.titulaciones:\n",
    "            if nombre == titulacion[\"Modalidad\"]:\n",
    "                print(titulacion[\"Contenido\"])\n",
    "                print(titulacion[\"Modalidad\"])\n",
    "                for facultad in titulacion[\"Facultad\"]:\n",
    "                    print(facultad, end=\" \")\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c830e4",
   "metadata": {
    "id": "81c830e4"
   },
   "source": [
    "Veamos cuantas Titulaciones tenemos con el tag **love**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281adbc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1663858097963,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "281adbc8",
    "outputId": "54ace7ac-a74b-4e0b-ce0d-c9dc6b1b9bf3"
   },
   "outputs": [],
   "source": [
    "Titulaciones.Facultad(\"Politecnica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80399b94",
   "metadata": {
    "id": "80399b94"
   },
   "source": [
    "Y del autor **Albert Einstein**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12479f89",
   "metadata": {
    "id": "12479f89"
   },
   "outputs": [],
   "source": [
    "Titulaciones.Modaliad(\"Presencial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfa125",
   "metadata": {
    "id": "d6cfa125"
   },
   "source": [
    "## Scrapeo de la web completa\n",
    "\n",
    "El programa está limitado a las 2 primeras páginas, voy a reescribir el código con un límite muy grande que garantice un scrapeo completo de la web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6f9418",
   "metadata": {
    "id": "fe6f9418"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrap_quotes(url=\"\"):\n",
    "    domain = \"https://uax.com\"\n",
    "    req = requests.get(f\"{domain}{url}\")\n",
    "    soup = BeautifulSoup(req.text)\n",
    "\n",
    "    titulaciones = []\n",
    "    titulaciones_facultades = soup.select(\"div.titulaciones\")\n",
    "    for titulacion_facultad in titulaciones_facultades:\n",
    "        titulacion = {}\n",
    "        titulacion['Contenido'] = titulacion_facultad.select(\"span.Contenido\")[0].getText()\n",
    "        titulacion['Modalidad'] = titulacion_facultad.select(\"small.Modalidad\")[0].getText()\n",
    "        titulacion['Facultad'] = []\n",
    "        for facultad in titulaciones_facultades.select(\"div.Facultad a.Facultad\"):\n",
    "            titulacion['Facultad'].append(facultad.getText())\n",
    "        titulaciones.append(titulacion)\n",
    "\n",
    "    next_url = None\n",
    "    link_tag = soup.select(\"li.next a\")\n",
    "    if len(link_tag) > 0:\n",
    "        next_url = link_tag[0]['href']\n",
    "\n",
    "    print(f\"Página {domain}{url}, {len(titulaciones)} titulaciones scrapeadas.\")\n",
    "\n",
    "    return titulaciones, next_url\n",
    "\n",
    "\n",
    "def scrap_site(limit=2):\n",
    "    todas_titulaciones = []\n",
    "    next_url = \"\"\n",
    "    while 1:\n",
    "        titulaciones, next_url = scrape_uax_titulaciones(next_url)\n",
    "        todas_titulaciones += titulaciones\n",
    "        limit -= 1\n",
    "        if limit == 0 or next_url == None:\n",
    "            return todas_titulaciones\n",
    "\n",
    "\n",
    "class Titulaciones:\n",
    "    titulaciones = []\n",
    "\n",
    "    if os.path.exists(\"titulaciones.csv\"):\n",
    "        with open(\"titulaciones.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for titulacion in data:\n",
    "                titulacion['Facultad'] = eval(titulacion['Facultad'])\n",
    "                titulaciones.append(titulacion)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        Titulaciones.titulaciones = scrape_website(\"https://uax.com\",limit=99) # <--- LIMITE MUY GRANDE\n",
    "        with open(\"titulaciones.csv\", \"w\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"Contenido\", \"Modalidad\", \"Facultad\"])\n",
    "            writer.writeheader()\n",
    "            for titulacion in Titulaciones.titulaciones:\n",
    "                writer.writerow(titulacion)\n",
    "\n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for titulacion in Titulaciones.titulaciones[:limite]:\n",
    "            print(titulacion[\"Contenido\"])\n",
    "            print(titulacion[\"Modalidad\"])\n",
    "            for facultad in titulacion[\"Facultad\"]:\n",
    "                print(facultad, end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def Facultad(nombre=\"\"):\n",
    "        for titulacion in Titulaciones.titulaciones:\n",
    "            if nombre in titulacion[\"Facultad\"]:\n",
    "                print(titulacion[\"Contenido\"])\n",
    "                print(titulacion[\"Modalidad\"])\n",
    "                for facultad in titulacion[\"Facultades\"]:\n",
    "                    print(facultad, end=\" \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def Modalidad(nombre=\"\"):\n",
    "        for titulacion in Titulaciones.titulaciones:\n",
    "            if nombre == titulacion[\"Modalidad\"]:\n",
    "                print(titulacion[\"Contenido\"])\n",
    "                print(titulacion[\"Modalidad\"])\n",
    "                for facultad in titulacion[\"Facultad\"]:\n",
    "                    print(facultad, end=\" \")\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cad9b3",
   "metadata": {
    "id": "02cad9b3"
   },
   "source": [
    "Vamos a ejecutar el scrapeo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b54b8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2232,
     "status": "ok",
     "timestamp": 1663858128767,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "81b54b8d",
    "outputId": "ca8e1971-92f4-4f59-885c-54ecfedc33a2"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scrape_website() missing 1 required positional argument: 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1a04d294a5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTitulaciones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrapear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-536794dcf6ac>\u001b[0m in \u001b[0;36mscrapear\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscrapear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mTitulaciones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitulaciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <--- LIMITE MUY GRANDE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titulaciones.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Contenido\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Modalidad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Facultad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scrape_website() missing 1 required positional argument: 'url'"
     ]
    }
   ],
   "source": [
    "Titulaciones.scrapear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72edb8",
   "metadata": {
    "id": "7b72edb8"
   },
   "source": [
    "Veamos cuantas Titulaciones encuentra ahora con el tag **love**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d6e9e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1663858142801,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "43d6e9e3",
    "outputId": "44b6efc5-9987-4aeb-a596-33ba05a0651d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "André Gide\n",
      "life love \n",
      "\n",
      "“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\n",
      "Marilyn Monroe\n",
      "friends heartbreak inspirational life love sisters \n",
      "\n",
      "“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\n",
      "Bob Marley\n",
      "love \n",
      "\n",
      "“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\n",
      "Elie Wiesel\n",
      "activism apathy hate indifference inspirational love opposite philosophy \n",
      "\n",
      "“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”\n",
      "Friedrich Nietzsche\n",
      "friendship lack-of-friendship lack-of-love love marriage unhappy-marriage \n",
      "\n",
      "“I love you without knowing how, or when, or from where. I love you simply, without problems or pride: I love you in this way because I do not know any other way of loving but this, in which there is no I or you, so intimate that your hand upon my chest is my hand, so intimate that when I fall asleep your eyes close.”\n",
      "Pablo Neruda\n",
      "love poetry \n",
      "\n",
      "“If you can make a woman laugh, you can make her do anything.”\n",
      "Marilyn Monroe\n",
      "girls love \n",
      "\n",
      "“The real lover is the man who can thrill you by kissing your forehead or smiling into your eyes or just staring into space.”\n",
      "Marilyn Monroe\n",
      "love \n",
      "\n",
      "“Love does not begin and end the way we seem to think it does. Love is a battle, love is a war; love is a growing up.”\n",
      "James Baldwin\n",
      "love \n",
      "\n",
      "“There is nothing I would not do for those who are really my friends. I have no notion of loving people by halves, it is not my nature.”\n",
      "Jane Austen\n",
      "friendship love \n",
      "\n",
      "“To love at all is to be vulnerable. Love anything and your heart will be wrung and possibly broken. If you want to make sure of keeping it intact you must give it to no one, not even an animal. Wrap it carefully round with hobbies and little luxuries; avoid all entanglements. Lock it up safe in the casket or coffin of your selfishness. But in that casket, safe, dark, motionless, airless, it will change. It will not be broken; it will become unbreakable, impenetrable, irredeemable. To love is to be vulnerable.”\n",
      "C.S. Lewis\n",
      "love \n",
      "\n",
      "“If I had a flower for every time I thought of you...I could walk through my garden forever.”\n",
      "Alfred Tennyson\n",
      "friendship love \n",
      "\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n",
      "Jane Austen\n",
      "humor love romantic women \n",
      "\n",
      "“To die will be an awfully big adventure.”\n",
      "J.M. Barrie\n",
      "adventure love \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titulaciones.Facultad(\"Ciencias Sociales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67abfce",
   "metadata": {
    "id": "c67abfce"
   },
   "source": [
    "Y cuantas del autor **Albert Einstein**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52d30296",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1663858148422,
     "user": {
      "displayName": "Rubén Juárez Cádiz",
      "userId": "13827813516298494191"
     },
     "user_tz": -120
    },
    "id": "52d30296",
    "outputId": "f7efe309-68cf-432a-de61-e4fe49b1ccc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "Albert Einstein\n",
      "change deep-thoughts thinking world \n",
      "\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "Albert Einstein\n",
      "inspirational life live miracle miracles \n",
      "\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "Albert Einstein\n",
      "adulthood success value \n",
      "\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "Albert Einstein\n",
      "simplicity understand \n",
      "\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "Albert Einstein\n",
      "children fairy-tales \n",
      "\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "Albert Einstein\n",
      "imagination \n",
      "\n",
      "“Any fool can know. The point is to understand.”\n",
      "Albert Einstein\n",
      "knowledge learning understanding wisdom \n",
      "\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "Albert Einstein\n",
      "life simile \n",
      "\n",
      "“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
      "Albert Einstein\n",
      "music \n",
      "\n",
      "“Anyone who has never made a mistake has never tried anything new.”\n",
      "Albert Einstein\n",
      "mistakes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titulaciones.Modalidad(\"Online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f3395",
   "metadata": {
    "id": "861f3395"
   },
   "source": [
    "Parece que todo funciona correctamente y podemos hacer tantas consultas como queramos sin repetir una y otra vez el proceso de scrapeo. En la práctica podríamos configurar un script que scrapee la página una vez al día para tener el fichero CSV sincronizado.\n",
    "\n",
    "En cualquier caso con esto acabamos este ejemplo y también la sección, espero que hayáis aprendido mucho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ca2c9",
   "metadata": {
    "id": "5a9ca2c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436587f-9431-4bf7-b418-0ccd047c3708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
